{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Approximate Value Iteration (AVI) Policy:\n","[1 1 1 1 1]\n","Approximate Policy Iteration (API) Policy:\n","[0 0 0 0 0]\n"]}],"source":["import numpy as np\n","\n","# AVI implementation\n","class ApproximateValueIteration:\n","    def __init__(self, state_dim, action_dim, feature_dim, gamma=0.9, epsilon=1e-6, max_iterations=1000):\n","        self.state_dim = state_dim\n","        self.action_dim = action_dim\n","        self.feature_dim = feature_dim\n","        self.gamma = gamma\n","        self.epsilon = epsilon\n","        self.max_iterations = max_iterations\n","        self.weights = np.zeros((action_dim, feature_dim))  # Initialize weights to zeros\n","\n","    def compute_q_values(self, state):\n","        # Compute Q-values for a given state using the weights\n","        q_values = np.dot(self.weights, state)\n","        return q_values\n","\n","    def train(self, feature_matrix, reward_matrix):\n","        for _ in range(self.max_iterations):\n","            prev_weights = np.copy(self.weights)  # Save the current weights\n","            for state_idx in range(self.state_dim):\n","                state = feature_matrix[state_idx]  # Get the feature vector for the current state\n","                q_values = self.compute_q_values(state)  # Compute Q-values for the current state\n","                best_action_value = np.max(q_values)  # Get the best action value\n","                for action_idx in range(self.action_dim):\n","                    reward = reward_matrix[action_idx, state_idx]  # Get the reward for the current action and state\n","                    bellman_residual = reward + self.gamma * best_action_value - q_values[action_idx]\n","                    self.weights[action_idx] += np.dot(state, bellman_residual)  # Update the weights\n","            if np.linalg.norm(prev_weights - self.weights) < self.epsilon:  # Check for convergence\n","                break\n","\n","    def get_policy(self, feature_matrix):\n","        policy = np.zeros(self.state_dim, dtype=int)  # Initialize the policy\n","        for state_idx in range(self.state_dim):\n","            state = feature_matrix[state_idx]  # Get the feature vector for the current state\n","            q_values = self.compute_q_values(state)  # Compute Q-values for the current state\n","            policy[state_idx] = np.argmax(q_values)  # Select the action with the highest Q-value\n","        return policy\n","\n","# API implementation\n","class ApproximatePolicyIteration:\n","    def __init__(self, state_dim, action_dim, feature_dim, gamma=0.9, epsilon=1e-6, max_iterations=1000):\n","        self.state_dim = state_dim\n","        self.action_dim = action_dim\n","        self.feature_dim = feature_dim\n","        self.gamma = gamma\n","        self.epsilon = epsilon\n","        self.max_iterations = max_iterations\n","        self.weights = np.zeros((action_dim, feature_dim))  # Initialize weights to zeros\n","\n","    def compute_q_values(self, state):\n","        # Compute Q-values for a given state using the weights\n","        q_values = np.dot(self.weights, state)\n","        return q_values\n","\n","    def compute_value_function(self, feature_matrix):\n","        value_function = np.zeros(self.state_dim)  # Initialize the value function\n","        for state_idx in range(self.state_dim):\n","            state = feature_matrix[state_idx]  # Get the feature vector for the current state\n","            q_values = self.compute_q_values(state)  # Compute Q-values for the current state\n","            value_function[state_idx] = np.max(q_values)  # Select the maximum Q-value as the value function\n","        return value_function\n","\n","    def train(self, feature_matrix, reward_matrix):\n","        for _ in range(self.max_iterations):\n","            prev_weights = np.copy(self.weights)  # Save the current weights\n","            for _ in range(self.max_iterations):\n","                prev_value_function = self.compute_value_function(feature_matrix)  # Compute the previous value function\n","                for state_idx in range(self.state_dim):\n","                    state = feature_matrix[state_idx]  # Get the feature vector for the current state\n","                    q_values = self.compute_q_values(state)  # Compute Q-values for the current state\n","                    policy = np.argmax(q_values)  # Select the action with the highest Q-value\n","                    reward = reward_matrix[policy, state_idx]  # Get the reward for the chosen action and state\n","                    bellman_residual = reward + self.gamma * prev_value_function[state_idx] - q_values[policy]\n","                    self.weights[policy] += np.dot(state, bellman_residual)  # Update the weights\n","                value_function = self.compute_value_function(feature_matrix)  # Compute the new value function\n","                if np.linalg.norm(prev_value_function - value_function) < self.epsilon:  # Check for convergence\n","                    break\n","            if np.linalg.norm(prev_weights - self.weights) < self.epsilon:  # Check for convergence\n","                break\n","\n","    def get_policy(self, feature_matrix):\n","        policy = np.zeros(self.state_dim, dtype=int)  # Initialize the policy\n","        for state_idx in range(self.state_dim):\n","            state = feature_matrix[state_idx]  # Get the feature vector for the current state\n","            q_values = self.compute_q_values(state)  # Compute Q-values for the current state\n","            policy[state_idx] = np.argmax(q_values)  # Select the action with the highest Q-value\n","        return policy\n","\n","# Example Usage and Output:\n","\n","# Define example data\n","state_dim = 5\n","action_dim = 2\n","feature_dim = 3\n","gamma = 0.9\n","epsilon = 1e-6\n","max_iterations = 1000\n","\n","feature_matrix = np.random.rand(state_dim, feature_dim)  # Randomly generate the feature matrix\n","reward_matrix = np.random.rand(action_dim, state_dim)  # Randomly generate the reward matrix\n","\n","# Instantiate and train AVI\n","avi = ApproximateValueIteration(state_dim, action_dim, feature_dim, gamma, epsilon, max_iterations)\n","avi.train(feature_matrix, reward_matrix)\n","\n","# Obtain AVI policy\n","avi_policy = avi.get_policy(feature_matrix)\n","print(\"Approximate Value Iteration (AVI) Policy:\")\n","print(avi_policy)\n","\n","# Instantiate and train API\n","api = ApproximatePolicyIteration(state_dim, action_dim, feature_dim, gamma, epsilon, max_iterations)\n","api.train(feature_matrix, reward_matrix)\n","\n","# Obtain API policy\n","api_policy = api.get_policy(feature_matrix)\n","print(\"Approximate Policy Iteration (API) Policy:\")\n","print(api_policy)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPJoAo4NCu2BhG2IBRTRN8B","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}
